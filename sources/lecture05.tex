\documentclass[11pt]{article}
\input{header.tex}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\begin{document}

\lecture{5: Dimensionality reduction}{ 30/03/2022}

\section{Dimensionality reduction recap}
Two versions of the JL lemma.
\begin{theorem}[Distributional JL]
For any integer $n$, and $0 < \varepsilon, \delta < \frac12$, and $m = \bigo(\log(1/\delta) / \eps^2)$ there is distribution $\mathcal{D}$ over matrices $\mathbb{R}^{m \times n}$ such that for every $x \in \mathbb{R}^n$ where $\|x\|_2 = 1$:
$$\Pr_{\Pi \sim \mathcal{D}} ( | \Pi \cdot x |_2^2 - 1  > \eps) < \delta $$
\end{theorem}
The second version:
\begin{theorem}[Metric JL]
For $X$, a set of $n$ points in dimension $n$, there exists linear $f : X \to \mathbb{R}^m$ for $m = \bigo(\log(n) / \eps^2)$ that preserves distances approximately, that is
$$\forall_{i,j}  |f(x_i) - f(x_j)|_2 \approx (1\pm\eps) |x_i - x_j|_2.$$
\end{theorem}

Theorem 2 follows from Theorem 1 by setting $\delta < 1/n^2$ and taking the union bound.


Those theorems are existential, but we know that  setting each matrix field to:
\begin{itemize}
\item $\mathcal{N}(0,1) \cdot 1/\sqrt{m}$ iid coefficients works
\item scaled rademacher $\{+1/-1\}   \cdot 1/\sqrt{m}$ iid coefficients works
\end{itemize}

Issue: applying a single projection takes $\bigo(nm)$ time.

\section{Fast JL \cite{DBLP:journals/siamcomp/AilonC09}}

Main idea is to structurize the matrix, so the time to apply matrix is $\bigo(m+n \log n)$.

We set $\Pi = \frac{1}{\sqrt{m}}  \cdot S \cdot H \cdot D$ where
\begin{itemize}
\item $S$ is $m \times n$ sampling matrix (each row has single 1 in a random position and 0's everywhere else, rows are independent)
\item $H$ is a $n \times n$ Fourier matrix or Hadamard matrix (we need $H H = I$ and $\max_{i,j} |H_{i,j}| \le 1/\sqrt{n} $)
\item $D$ is a $n \times n$ diagonal matrix $\textrm{diag}(\sigma)$ where $\sigma$ is a vector of independent Rademachers
\end{itemize}

$D$ is applied in time $\bigo(n)$, $H$ is applied in time $\bigo(n \log n)$ and $S$ is applied in time $\bigo(m)$.



\begin{theorem}
For $m = \bigo(\log(1/\delta) \cdot \log(n/\delta) \cdot \eps^{-2})$ and $\|x\|_2 = 1$ we have 
$$\Pr_{\Pi} ( | \Pi \cdot x |_2^2 - 1  > \eps) < \delta $$
\end{theorem}

We need:
\begin{theorem}[Khintchine inequality]
For any $p \ge 1$, $x \in \mathbb{R}^n$ and $(\sigma_i)$ independent Rademacher,
$$ \left( \E \left|\sum_i x_i \sigma_i\right|^p \right)^{1/p} \le \bigo(\sqrt{p}) \| x \|_2 $$
\end{theorem}
\begin{theorem}[Chernoff bound]
$X_1,\ldots,X_n$ are independent random variables and $X_i \in [0,\tau]$. Let $\mu = \E \sum_i X_i $. Then
$$\Pr[ | \sum_i X_i - \mu| > \eps \mu ] < 2 \exp(- \frac{\eps^2 \mu}{2 \tau} )$$
\end{theorem}

\begin{proof}[Proof of the main result]
Denote $y = \frac{1}{\sqrt{n}}HDx$ and $z = \sqrt{\frac{n}{m}} \cdot S \cdot y$.


First our goal is to show  bound on $\| y \|_{\infty}$.

$$y_i = (\frac{1}{\sqrt{n}}HDx)_i = \sum_{j=1}^n \sigma_j \frac{1}{\sqrt{n}} \gamma_{i,j} x_j = \sigma \odot w^{(i)}$$
where $w^{(i)}$ is a vector $w^{(i)}_j = \frac{1}{\sqrt{n}} \gamma_{i,j}x_j$.

First, $\|y\|_2 = \|x\|_2 = 1$ since (normalized) Hadamard transform preserves $L_2$ norms, but one can also prove that $\E |y_i|^2 = \E | \sigma \odot w^{(i)} |^2  = \|w^{(i)}\|_2^2 = \sum_j (\frac{1}{\sqrt{n}} \gamma_{i,j} x_j)^2 = \frac{1}{n} \|x\|_2 = \frac{1}{n}$ where we only used that $H$ has $-1/+1$ coefficients and that $\sigma$ is Rademacher.


By Khintchine inequality, and using bound on length of $x$ and fact that $\gamma_{i,j} \in \{-1,+1\}$, for some absolute constant $C$:$$\E |y_i|^p \le \left(C \sqrt{p} \| w^{(i)} \|_2\right)^{p} = \left(\sqrt{\frac{\bigo(p)}{n}}\right)^p$$
by Markov's inequality:
$$\Pr\left[ |y_i| \ge \sqrt{\frac{\bigo(p)}{n}} \cdot \left( \frac{2n}{\delta} \right)^{1/p}\right] = \Pr\left[ |y_i|^p \ge  \left(C \cdot \sqrt{\frac{p}{n}}\right)^p \cdot \frac{2n}{\delta} \right] \le \frac{\delta}{2n}$$

Optimizing, the term
$$\varphi(p) =  \sqrt{\frac{\bigo(p)}{n}} \cdot \left( \frac{2n}{\delta} \right)^{1/p} \sim \exp\left(\bigo(1)+\frac{1}{2} \ln p + \frac{1}{p} \ln \left( \frac{2n}{\delta} \right) \right)$$
minimizes when $\frac{1}{2}\frac{1}{p} - \frac{1}{p^2} \ln(2n/\delta)=0$ (the derivative of the exponent) or equivalently $p =  2 \ln \left( \frac{2n}{\delta} \right) $, so then $ \varphi_{\min}(p)= \sqrt{\frac{\bigo(p)}{n}}\cdot \sqrt{e}$ and so
$$\Pr\left[ |y_i| \ge  \bigo\left( \sqrt{\frac{\ln (2n/\delta)}{n}} \right)\right] \le \frac{\delta}{2n},$$
and taking the union bound
$$\Pr\left[ \|y\|_\infty \ge  \bigo\left(\sqrt{\frac{\ln (2n/\delta)}{n}}\right) \right] \le \frac{\delta}{2}.$$
So in the following we condition on the event that $\| y \|_\infty = \bigo\left(\sqrt{\frac{\ln (2n/\delta)}{n}}\right)$.

Now consider $X_i = (z_i)^2$ as a random variable. We have the following:
$$\E X_i = \E (z_i)^2 = \frac{n}{m} \cdot \frac{\|y\|_2^2}{n} = \frac{1}{m}.$$
And from bound on $\|y\|_\infty$ there is $X_i \le \frac{n}{m} \cdot \bigo(\frac{\ln(2n/\delta)}{n} )= \bigo(\frac{\ln(2n/\delta)}{m}) = \tau$.


We now apply Chernoff bound, with $\mu = 1$ (keep in mind that $\sum_i X_i = \|z\|_2^2$)

$$\Pr[ | \sum_i X_i - 1| > \eps  ] < 2 \exp(- \frac{\eps^2 }{2 \tau} ) = 2 \exp\left(- \frac{\eps^2 m}{\bigo(\ln(2n/\delta)) }\right)$$


Now, usually when applying Chernoff bound, $\tau=1$ and then it is enough to set $m = \bigo(\varepsilon^{-2} \log (1/\delta))$ to have the probability in the bound be $\delta$. Unfortunately in our case, $\tau \gg 1$ so we have to offset for the log term in the denominator. So we need to set 
$m = \bigo(\varepsilon^{-2} \log (1/\delta) \log(n/\delta)$ so we can bound the probability in the Chernoff by $\delta/2$.

Taking union bound over both $\delta/2$ failure probability finishes the proof.
\end{proof}

Using this approach, this dependency is roughly optimal (that is, we are losing one log vs. optimal JL). However, one trick to reduce $\eps$ to optimal at the cost of slower runtime is to apply $m' \times m$ ''naive'' JL at the end of the chain of matrices, for $m' = \bigo(\log (1/\delta) \eps^{-2})$. This adds $\bigo(m' \cdot m) = \bigo(\eps^{-4} \log^2 (1/\delta) \log (n/\delta))$ to the running time though.

\section{Sparse JL \cite{DBLP:conf/stoc/DasguptaKS10}}

Motivation: if $x$ is sparse (that is, $\|x\|_0$ is small), we expect time proportional to $\|x\|_0$. 

We consider distributions $\mathcal{D}$ of matrices such that:
\begin{itemize}
\item Each column has only $s$ non-zero elements, for some $s$. (Either deterministically or in expectation.)
\item They still provide good dimensionality reduction.
\end{itemize}

The time to compute $\Pi x$ is then $s \cdot \|x\|_0$, since each column determines ''where'' each $x_i$ contributes and can be processed in $\bigo(s)$ time.

\subsection{Dasgupta etal. construction}
$s = \bigo(\eps^{-1} \log (1/\delta) \log^2 (m/\delta))$,\\ $h : [sn] \to [m]$ be a random hash function , and let
$H \in \{-1,0,1\}^{m \times sn}$ be such that $H_{ij} = \delta_{i,h(j)} r_j$. (all $r$ are indep. Rademacher).
$P \in \{0,1\}^{sn \times n}$ be such that
$$P_{i,j} = \begin{cases}1 \text{ for } (j-1)s + 1 \le i \le js\\0 \end{cases}$$

Intuition:
$P$ creates $s$ copies of each element of input, $H$ hashes each element (after duplication) into $[m]$ together with $+1/-1$ coef. This can be evaluated implicitly without expanding the matrices.


\begin{theorem}
$\Pi = \frac{1}{\sqrt{s}}HP$ has JL guarantees. $\Pi x$ can be evaluated in time $\bigo(s \|x\|_0)$.
\end{theorem}
We skip the proof.

\subsection{Kane, Nelson construction}
$s = \bigo(\eps m) = \bigo(\varepsilon^{-1} \log(1/\delta))$

Construction 1: matrix $m \times n$, where in each column we place $s$ random $-1/+1$ (sample without replacement), normalized with $\frac{1}{\sqrt{s}}$ coef.\\
Construction 2: group each column into $s$ blocks, each of size $m/s$. Pick in each block one $-1/+1$, normalize with $\frac{1}{\sqrt{s}}$ coef.\\

Construction 2 is effectively the same as \textsf{CountSketch}. The proof that it works shows that analysis using more than just 2-independence can show a very good concentration (CountSketch uses median, here we can use average).


\section{Missing proofs}
\begin{proof}[Proof of Khintchine Inequality]
For any variable $X$, $\E[|X|^p]^{1/p}$ is increasing with $p$ (see: generalized average inequality), so we can round-up $p$ to even integer. Consider $g_i \sim \mathcal{N}(0,1)$.
Expand $\E[ (\sum_i \sigma_i x_i)^p]$ into sum of monomials. Any monomial with odd-exponents vanishes. Similarly in $\E[ (\sum_i g_i x_i)^p ]$. For any even-exponents $\alpha_1,\ldots$, $\E \prod_i \sigma_i^{\alpha_i} = 1$ while $\E \prod_i g_i^{\alpha_i} \ge 1$, so the gaussian case dominates the rademacher case.\footnote{Useful fact is that for $g \sim \mathcal{N}(0,1)$ and even $p$, $\E[g^p] = (p-1)!!$.}

But $\sum_i g_i x_i$ is itself normal variable $\mathcal{N}(0,\|x\|_2^2)$, so
$$\E[ (\sum_i \sigma_i x_i)^p ] \le \E[ (\sum_i g_i x_i)^p ] =  (p-1)!! \|x\|_2^p$$
Asymptotically, $((p-1)!!)^{1/p} \approx (2^{p/2} \cdot (p/2)!)^{1/p} \approx \sqrt{2} \cdot \left(\frac{(p/2)^{p/2}}{e^{p/2}} \right)^{1/p} = \sqrt{\frac{p}{e}}$, so the hidden constant in Khintchine inequality is $\bigo(\sqrt{p})$.

\end{proof}

\bibliographystyle{alpha}
\bibliography{bib}
\end{document}


