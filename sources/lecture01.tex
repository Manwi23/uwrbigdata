\documentclass[11pt]{article}
\input{header.tex}

\begin{document}

\lecture{1: Approximate Counting, Distinct Elements}{ 02/03/2022}

\section{Approximate counting} 
The problem is to maintain a counter that supports following operations:
\begin{align*}
\text{reset(),  } &[n\leftarrow 0]\\
\text{inc(),    } &[n\leftarrow n+1]\\
\text{query(),  } &[\text{output n}]
\end{align*}

Simple lower bound of $\log(n)$ bits for exact (information-theoretic lower bound).

\paragraph{Goal:} algorithm that queried outputs $n’$ such that $\Ppb( |n-n'| > \varepsilon n ) < \delta$.
\subsection{Morris’ algorithm \cite{DBLP:journals/cacm/Morris78a}}
\paragraph{Local state:} X [int], represents $n \sim 2^X$. The crucial part of algorithm is to design how we increase $X$.

\paragraph{Inc: }$X \leftarrow X+1$ with some small probability ($\sim 2^{-X}$), with intuition being that the ppb of $n$ being exactly $2^{X+1}-1$ is $2^{-X}$.

Let us analyze increment probability = $2^{-X}$. Let $X_n$ be random variable denoting state of algorithm after $n$ increases.
\begin{theorem}
\begin{align}
    \Es[2^{X_0}] &= 2^{X_0} = 1 \\
    \Es[2^{X_n}] &= n+1 \text{  by induction}
\end{align}
\end{theorem}
\begin{proof}
\begin{align*} \Es 2^{X_{n+1}} &=\sum_{j=0}^{\infty} \Ppb \left(X_{n}=j\right) \cdot \Es \left(2^{X_{n+1}} | X_{n}=j\right) \\ &=\sum_{j=0}^{\infty} \Ppb \left(X_{n}=j\right) \cdot\left(2^{j}\left(1-\frac{1}{2^{j}}\right)+\frac{1}{2^{j}} \cdot 2^{j+1}\right) \\ &=\sum_{j=0}^{\infty} \Ppb \left(X_{n}=j\right) 2^{j}+\sum_{j} \Ppb \left(X_{n}=j\right) \\ &=\Es 2^{X_{n}}+1 \\ &=(n+1)+1 \end{align*}
\end{proof}

Morris algorithm output: $Z = 2^{X_{n}}-1   \leftarrow$, which is an unbiased estimator of $n$ (that is $\Es[Z] = n$).

\subsubsection{Analysis of variance to extract guarantees:}

\begin{theorem}
We show inductively that $\Es[2^{2X_n}] = 3/2 n^2 + 3/2 n + 1$.
\end{theorem}
\begin{proof}
see exercise
\end{proof}

Since
\begin{align*}
    \Var[Z] &= \Var[2^{X_n}] \\
    &= \Es[2^{2 X_n} ] - (\Es[2^{X_n}])^2 \\
    &= \frac{3}{2} n^2 + 3/2 n + 1 - (n+1)^2 \\
    &= \frac{1}{2} n^2 - \frac{1}{2}n,
\end{align*}
by Chebyshev’s inequality $\Ppb( |Z-n| > \varepsilon n) \leq 1/(2 \varepsilon^2)$.

This only gives failure probability $\delta < \frac{1}{2}$ for $\varepsilon > 1$, which is not very informative: (large) constant approximation with constant probability. But that was to be expected: our algorithm only outputs powers of two, so it cannot do much better job.
\subsection{Morris+}
Repeat $k$ times independently, take average of estimations.
Since variance is additive: $\Var(Z') = \frac{1}{k^2}  (\Var(Z_1) + \Var(Z_2) + \dots + \Var(Z_k)) = 1/k  \Var(Z)$ so number of iterations necessary becomes: $k = \bigo(\frac{1}{\varepsilon^2 \delta})$
(ok for 9/10 ppb of correctness, bad for whp correctness).

\subsection{Morris++}
Run $t$ copies of Morris+ algorithm, each with $\delta = \frac{1}{3}$ and take median of estimations as a final estimation. Each estimation is ok with probability $\geq \frac{2}{3}$, so for the median to fail at least $\frac{1}{6}$ fraction of estimations need to fail (all too large or all too small)
Chernoff bound gives us:
\begin{equation}
\Ppb \left(\sum_{i=1}^{t} Y_{i} \leq \frac{t}{2}\right) \leq \Ppb \left(\left|\sum_{i=1}^{t} Y_{i}-\Es \sum_{i=1}^{t} Y_{i}\right| \geq \frac{t}{6}\right) \leq 2 e^{-t / 3}<\delta
\end{equation}
for $t = \Theta(\lg (1 / \delta))$.
Final \textbf{bit} complexity $\bigo(\log \log (n/(\varepsilon \delta)) \frac{1}{\varepsilon^2} \log(\frac{1}{\delta}))$.

\paragraph{Lower bound:} $\Omega(\log \log_{1+\varepsilon} n) = \Omega(\log(1/\varepsilon) + \log \log n)$ (for $\delta=0$, its trickier to prove lowerbound involving $\delta$)


\section{Distinct elements}

\paragraph{Input:} Stream of values $i_1, i_2, …, i_m$ from $[n]$
query() $\leftarrow$ number of distinct elements

\paragraph{Trivial solution:} remember the stream, bitvector
\subsection{Flajolet Martin \cite{DBLP:journals/jcss/FlajoletM85}}
Pick a hash function $h: [n] \rightarrow [0,1]$     (for a moment let us assume ideal real numbers, and perfectly random hash function).

\begin{enumerate}
\item initially $Z=1$
\item input $X$: $Z = \min(Z, h(X))$
\item estimator: $Y = 1/Z - 1$
\end{enumerate}

\begin{observation}
Repeats do not affect Z.
\end{observation}

If $t$ is the number of distinct elements, then $Z = \min(r_1, r_2, \dots, r_t)$ where $r_i$ are all independent and from $[0,1]$.

\begin{lemma}
\begin{equation}
    \Es[Z] = \frac{1}{t+1}
\end{equation}
\end{lemma}
\begin{proof}
Pick fresh $A$ at random from $[0,1]$. By symmetry, 
$$\Es[Z] = \Ppb[A<Z] = \Ppb[A\text{ is minimal among }{A,r_1,\dots,r_t}] = \frac{1}{(t+1)}.$$
\end{proof}
\begin{lemma}
\begin{equation}
\Es[Z^2] \leq \frac{2}{(t+1)(t+2)}
\end{equation}
\end{lemma}
\begin{proof}
Pick fresh $A, B$ at random from $[0, 1]$. By symmetry, $\Es[Z^2] = \Ppb[A<Z \wedge B<Z] = \frac{2}{(t+1)(t+2)}$ 
\end{proof}
\begin{proof}[Alternative proof]
\begin{align*}
\Es\left[Z^{2}\right] &=\int_{0}^{\infty} \Ppb\left(Z^{2}>\lambda\right) d \lambda \\
&=\int_{0}^{\infty} \Ppb(Z>\sqrt{\lambda}) d \lambda \\
&=\int_{0}^{1}(1-\sqrt{\lambda})^{t} d \lambda \\
&=2 \int_{0}^{1} u^{t}(1-u) d u \quad[u=1-\sqrt{\lambda}] \quad=\frac{2}{(t+1)(t+2)} \end{align*}
\end{proof}
\begin{equation}
\Var[Z]=\frac{2}{(t+1)(t+2)}-\frac{1}{(t+1)^{2}}=\frac{t}{(t+1)^{2}(t+2)}<(\Es[Z])^{2}
\end{equation}

\begin{remark}
Applying Chebyshev's inequality $\rightarrow$ results in a guarantee of a (large) constant approximation with lets say $\frac{9}{10}$ probability.
\end{remark}

\paragraph{Issue:} $\Es[\frac{1}{Z}] \neq \frac{1}{\Es[Z]}$, but concentrating $Z$ with $1+\varepsilon$ multiplicative error will give $1+\varepsilon$ multiplicative error for $\frac{1}{Z}$.

\subsection{FM+}
To reach better approximation guarantee, we need to concentrate our output around expected value.
\paragraph{Approach 1} copy approach from Morris’ algorithm - ``repeat k times and take average''
to improve variance, set $k = \bigo(\frac{1}{\varepsilon^2})$ for $\frac{9}{10}$ probability of $1+ \varepsilon$ approximation.

\paragraph{Approach 2} replace ``take minimum'' with ``take k-th smallest value''
(to be analyzed $\rightarrow$ exercise).

\subsection{FM++}
To improve probability of success, repeat FM+ algorithm $t = \bigo(\log \delta^{-1})$ times, and take median of answers. This boosts probability of success to $1-\delta$.

Total memory complexity is

$\bigo(\log n  \frac{1}{\varepsilon^2} \log \delta^{-1})$ of \textbf{words} (each word is $\log n$ bits).

\subsection{Issues}

\paragraph{Recall} ``for a moment let us assume ideal real numbers''.

We only care about relative order of hashes, and use actual value as an estimator. Using hash-functions of form $h: [n] \rightarrow \{\frac{0}{M}, \frac{1}{M}, \dots, \frac{M-1}{M}, \frac{M}{M}\}$ for some $M = n^3$, as it only introduces small relative error (whp each hash is $\geq \frac{1}{n}$ thus relative error introduced is at most $(1+\frac{1}{n})$, and wlog $\varepsilon>\frac{1}{n}$), and whp there are no collisions of hashes.

\paragraph{Recall} ``and perfectly random hash function''.

Randomness vs. pseudorandomness $\rightarrow$ c.f. exercises

\section{Further reading}
\begin{itemize}
    \item hyperloglog algorithm, which very efficient in theory and practice, but has extremely nontrivial analysis \cite{DBLP:conf/esa/DurandF03} \cite{DBLP:conf/edbt/HeuleNH13}
    \item\ \cite{DBLP:conf/soda/Blasiok18} - optimal $\Theta(\log{n} + \frac{\log{\delta^{-1}}}{\varepsilon^2})$ bits.

\end{itemize}

\bibliographystyle{alpha}
\bibliography{bib}


\appendix
\section{Probability recap}
\begin{definition}
\begin{enumerate}
    \item The empty set is an event, $\varnothing \in \sF$
    \item Given a countable set of events $A_1, A_2, \dots$, its union is also an event, $\bigcup_{i\in\Nat}A_i \in \sF$
    \item if $A$ is an event, then so is the complementary set $A^c$
\end{enumerate}
\end{definition}

\begin{definition}
\begin{enumerate}
    \item $\Ppb(\emptyset)=0, \Ppb(\Omega)=1$
    \item if $A_1, A_2, \dots$ are mutually excluding events, then $\Ppb\left(\cup_{i=1}^{\infty} A_{i}\right)=\sum_{i=1}^{\infty} \Ppb\left(A_{i}\right)$
\end{enumerate}
A $\Ppb: \mathcal{F} \mapsto[0,1]$ satisfying these is called a probability.

The triple $(\Omega, \mathcal{F}, \Ppb)$ is called a probability space.
\end{definition}

\begin{definition}
We define conditional probability as 
\begin{equation*}
    \Ppb(A | B)=\frac{\Ppb(A \cap B)}{\Ppb(B)}
\end{equation*}
\end{definition}
\begin{theorem}
Let $B_{1}, \dots, B_{n}$ be a partition of $\Omega$, then
\begin{equation}
\Ppb(A)=\sum_{i=1}^{n} \Ppb\left(A | B_{i}\right) \Ppb\left(B_{i}\right)
\end{equation}
\end{theorem}
\begin{definition}
Events A and B are called independent if
\begin{equation}
\Ppb(A \cap B)=\Ppb(A) \Ppb(B).
\end{equation}
When$ 0 < \Ppb(B) < 1$, this is the same as
\begin{equation}
\Ppb(A | B)=\Ppb(A)=\Ppb\left(A | B^{c}\right)
\end{equation}
A family $\{A_i : i \in I\}$ of events is called independent if
\begin{equation}
\Ppb\left(\cap_{i \in J} A_{i}\right)=\prod_{i \in J} \Ppb\left(A_{i}\right)
\end{equation}
for any finite subset J of I.
\end{definition}

\begin{definition}
A random variable is
Informally: A quantity which is assigned by a random experiment.
Formally: A mapping $X : \Omega \rightarrow \Real$.
\end{definition}
\begin{definition}
The cumulated distribution function(cdf) is:
\begin{equation}
F(x)=\Ppb(X \leq x)
\end{equation}
If satisfies following properties:
\begin{enumerate}
    \item $\lim _{x \rightarrow-\infty} F(x)=0, \lim _{x \rightarrow+\infty} F(x)=1$
    \item $x<y \Rightarrow F(x) \leq F(y)$
    \item $F$ is right-continuous, ie. $F(x+h) \rightarrow F(x)$ as $h \downarrow 0$
\end{enumerate}
\end{definition}
\begin{definition}
The mean of a stochastic variable is
$$
\Es X=\sum_{i \in \mathbb{Z}} i \Ppb(X=i)
$$
in the discrete case, and
$$
\Es X=\int_{-\infty}^{+\infty} f(x) d x
$$
in the continuous case. In both cases we assume that the
sum/integral exists absolutely.
The variance of $X$ is
$$
\Var X=\Es(X-\Es x)^{2}=\Es X^{2}-(\Es X)^{2}
$$
\end{definition}

\begin{definition}
The conditional expectation is the mean in the conditional distribution
\begin{equation}
\Es(Y | X=x)=\sum_{y} y f_{Y | X}(y | x)
\end{equation}
It can be seen as a stochastic variable: Let $\psi(x)=\Es(Y | X=x)$,
then $\psi(X)$ is the conditional expectation of $Y$ given $X$
\begin{equation}
\psi(X)=\Es(Y | X)
\end{equation}
We have
\begin{equation}
\Es(\Es(Y | X))=\Es Y
\end{equation}
\end{definition}

\begin{definition}
Conditional variance $\Var(Y|X)$ is the variance in the conditional distribution.
\begin{equation}
\Var(Y | X=x)=\sum_{y}(y-\psi(x))^{2} f_{Y | X}(y | x)
\end{equation}

This can also be written as
$$
\Var(Y | X)=\Es\left(Y^{2} | X\right)-(\Es(Y | X))^{2}
$$
and can be manipulated into
$$
\Var=\Es \Var(Y | X)+\Var \Es(Y | X)
$$
which partitions the variance of $Y$.
\end{definition}

\begin{theorem}[Markov's inequality]
Let $X \geq 0$ be a random variable. Then for any $k \geq 1:$
\begin{equation}
\Ppb(X \geq k \cdot \Es[X]) \leq \frac{1}{k}
\end{equation}
\end{theorem}
\begin{theorem}[Chebyshev's inequality]
Let $X$ be a random variable. For any $k>0$:
\begin{equation}
\Ppb(|X-\Es[X]| \geq k \cdot \sqrt{\Var[X]}) \leq \frac{1}{k^{2}}
\end{equation}
\end{theorem}
\begin{theorem}[Hoeffding bound]
Let $X_{1}, X_{2}, \ldots, X_{n} \in\{0,1\}$ be fully independent random variables. Let $X=\sum_{i} X_{i} .$ Then:
\begin{equation}
    \Ppb(|X-\Es[X]| \geq t) \leq 2 \exp \left(-\frac{t^{2}}{n}\right)
\end{equation}
\end{theorem}
\end{document}
