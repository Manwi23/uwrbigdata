\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage[right=0.8in, top=1in, bottom=1.2in, left=0.8in]{geometry}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Blue,linkcolor=BrickRed]{hyperref}
\usepackage[dvipsnames,usenames]{xcolor}
%\spacing{1.06}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{\vspace{0.25cm}
      \hbox to 5.78in { {University of Wrocław:\hspace{0.12cm}Algorithms for
          Big Data (Fall'19)} \hfill #2 }
      \vspace{0.48cm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{0.42cm}
      \hbox to 5.78in { {#3 \hfill #4} }\vspace{0.25cm}
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe:\hspace{0.08cm}#4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{property}[theorem]{Property}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\newcommand{\bigo}{\mathcal{O}}
\setcounter{MaxMatrixCols}{20}

\begin{document}

\lecture{13: MPC}{ 20/01/2020}{Lecturer: \emph{Przemysław Uznański }}{-}

\section{Massively Parallel Computing \cite{DBLP:conf/spaa/LattanziMSV11}}
Modern distributed computing model (captures map-reduce, hadoop, spark, etc.).

\begin{itemize}
\item input of size $n$
\item $k$ machines, each of space $S$, $S = n^{1-\delta}$, $k \cdot S = \bigo(n)$.
\item output (might be too large, e.g. size $n$ that does not fit on a single machine)
\end{itemize}

Computation:
\begin{itemize}
\item computation happens over $R$ rounds
\item each machine: near linear computation per round, so total computation cost $\bigo(n^{1+o(1)} R)$
\item each machine communicates $\sim S$ bits per round, so total communication cost $\bigo(n R)$
\end{itemize}
goal: minimize $R$

\subsection{Sorting (Tera-Sort)}
Intuition: if we partition input onto machines, so each machine receives contiguous fragment of size $S$, then we are done in a single round (each machine sorts and outputs its own part, output == concatenation of outputs).

Idea: 
\begin{itemize}
\item each machine receives input
\item each machine samples randomly from its input
\item each sample is sent to single machine ($1$ round)
\item 1 machine gathers all the samples, sorts locally, and sents back to everyone approximate histogram
\item machines use approximate histogram to decide how to partition locally their input and sent it to proper receivers
\item then everyone sorts their parts
\end{itemize}

Total sample size $s = \bigo(S)$, and whp histogram is with $\pm \varepsilon n$ error, where $\frac{\log n}{\varepsilon^2} \le s$. We are ok if error is $\bigo(S)$. This is satisfied when $S^{3/2} = \Omega(n \sqrt{\log n})$, or $S = \widetilde\Omega(n^{2/3})$, or $k = \widetilde\bigo(n^{1/3})$ $\implies$ sorting in $\bigo(1)$ rounds


\section{Connectivity}
Input: list of edges, partitioned (arbitrarily), size $N$
output: each vertex labeled with id of connected component

Notation: id of vertex $\pi(v)$, label of vertex $\ell(v)$, $\Gamma(v)$ denotes neighbourhood
Approach:
\begin{itemize}
\item let $L_v$ be the set of vertices $\{u : \ell(u) = \ell(v)\}$ -- always a subset of connected component of $v$
\item initially $\ell(v) \gets \pi(v)$
\item some vertcies are called \emph{active}
\item every $L_v$ will have exactly one active vertex (e.g. one with smallest $\ell$), that makes decisions wrt whole $L_v$, assume cannonically its $v$
\item in each round:
\begin{itemize}
\item each active vertex becomes a \emph{leader} with ppb $1/2$
\item if $v$ is a leader, mark whole $L_v$ as a leaders
\item every active non-leader $v$, find $w \in \Gamma(L_v)$ that is a leader and minimizes $\pi(w)$
\item whole $L_v$ joins $L_w$
\end{itemize}
\end{itemize}

Observation: each round decreases \emph{in expectation} number of components by a factor of $1/4$, so $\bigo(\log n)$ rounds are enough. Implementation details:
\begin{itemize}
\item edges are stored locally
\item split vertices of large degree into smaller vertices of degree $\bigo(S)$ (this is already non-trivial and requires e.g. sorting)
\item ditributed data structure implementing vertex info required (implemented e.g. via sorting, emulates sending messages over the edges etc.)
\end{itemize}


\section{MST}
Input: graph of $N = n^{1+\delta}$ edges. Output: some approximation of a maximal matching. Output fits into single machine (one edge per vertex). Assume $S = n^{1+\varepsilon}$.
\begin{itemize}
\item Split output randomly into machines.
\item Each machine $i$ computes MST of its input $T_i$
\item Recurse on $\bigcup_i T_i$
\end{itemize}

Filtering step reduces total number of edges from $n^{1+\delta}$ to $n^{1+\delta-\varepsilon}$, thus the number of rounds is $\bigo(\delta/\varepsilon)$.


\section{Maximal matching}
(Setting the same as in previous algorithm)

\begin{itemize}
\item Sample $E'$, set of edges of size $\bigo(S)$. 
\item Send $E'$ to one machine. Compute $M'$, the maximal matching of $E'$.
\item Remove all vertices from $V[E']$ from graph. Recurse on remaining graph. Call $M$ the result of recursion.
\item Return $M \cup M'$.
\end{itemize}

Correctness: simulated greedy. 
\begin{lemma}If $E'$ is a set of edges picked by sampling with probability $p$. Let $I$ be set of vertices not adjacent to $E'$. With very high probability, $|E[I]| \le 2n/p$. 
\end{lemma}
\begin{proof}
Follows from concentration bounds, since if we pick set $E_1$ of size $2n/p$, then expected intersection size $\mathbb{E} | E[I] \cap E' | = 2n$, so with exponentially small probability its not empty. We then take union bound over all possible subsets.
\end{proof}
\begin{theorem}
Number of rounds is $\bigo(\delta/\varepsilon)$.
\end{theorem}
\begin{proof}
Sampling probability needs to be $p = S/m = n^{\varepsilon-\delta}$. So in each step number of edges is reduced from $n^{1+\delta}$ to $2n^{1+\delta-\varepsilon}$.
\end{proof}

\section{Maximal matching, approach 2}
\begin{itemize}
\item Partition edges randomly into machines. 
\item Each machine receives $G_i$.
\item Each machine computes $M_i$, maximal matching of $G_i$.
\item Everyone sends $M_i$ to a coordinator machine.
\item Coordinator outputs $M$, maximal matching from $\bigcup_i M_i$.
\end{itemize}

The limiting factor is that all of the maximal-matchings need to be aggregated on a single machine, so $S \ge n \cdot \frac{N}{S}$, or $S \ge \sqrt{n \cdot N}$, which for dense graphs is $n^{3/2}$.

Claim: algorithm outputs maximal matching of size $\Theta(n)$, so its $\Theta(1)$-approximation of MM.

\begin{proof}
Consider greedy algorithm going through edges in order of $M_1,\ldots,M_k$. W.l.o.g. maximal matching is of size $\bigo(n)$. Consider step $i$ (processing $M_i$). 

\begin{lemma}
Assume we have already selected $o(n)$ edges from $M_1 \cup \ldots \cup M_{i-1}$, as otherwise we are done.\\
$E_{\text{old}}$ -- all edges in $G_i$ that are adjacent to already selected vertex\\
$\mu_{\text{old}}$ -- size of a maximum matching in $G_i$ using only edges in $E_{\text{old}}$.\\
There exists matching in $G_1 \cup \ldots \cup G_{i}$ of size $\mu_{\text{old}} + \Omega(n/k)$.
\end{lemma}
\begin{proof}
$G$ contains a matching of size $\Theta(n) - 2 \mu_{\text{old}} = \Theta(n)$ that is not adjacent to any of the vertices from already selected maching. By random partitioning, $\Theta(n/k)$ of those edges land in $G_i$, so $G_i$ contains matching of size $\mu_{\text{old}} + \Omega(n/k)$.
\end{proof}


So any maximal matching in $G_i$ needs to be of size at least $\mu_{\text{old}} + \Omega(n/k)$. At most $\mu_{\text{old}}$ of edges in any maximal matching can be adjacent to vertices already blocked in previous rounds, so from maximal matching we are getting $\Omega(n/k)$ new edges.
\end{proof}

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}


