\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage[right=0.8in, top=1in, bottom=1.2in, left=0.8in]{geometry}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Blue,linkcolor=BrickRed]{hyperref}
\usepackage[dvipsnames,usenames]{xcolor}

%\spacing{1.06}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{fcit
    \vbox{\vspace{0.25cm}
      \hbox to 5.78in { {University of Wrocław:\hspace{0.12cm}Algorithms for
          Big Data (Fall'19)} \hfill #2 }
      \vspace{0.48cm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{0.42cm}
      \hbox to 5.78in { {#3 \hfill #4} }\vspace{0.25cm}
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe:\hspace{0.08cm}#4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\E}{{\mathbb E}}
\DeclareMathOperator{\var}{Var}
\newcommand{\eps}{\varepsilon}
\newcommand{\bigo}{\mathcal{O}}
\setcounter{MaxMatrixCols}{20}

\begin{document}

\lecture{7: Regression, Low Rank Approximation}{ 25/11/2019}{Lecturer: \emph{Przemysław Uznański }}{-}
\section{Linear Regression}
In a regression problem we have predictor variables $a_1, \ldots,a_d$ and a measured variable $b$. In linear regression, we assume there is a relation $b \approx \sum_i a_i x_i$ for some $x_1,\ldots,x_d \in \mathbb{R}$.

We assume we received $n$ batches $(a_{i,1},\ldots,a_{i,d},b_i), i = 1..n$. In the least square method we minimize 
$$\sum_{i} (a_{i,1}x_1 + \ldots a_{i,d} x_d - b_i)^2.$$
Formally:
\begin{definition}
On the input we have $A = \mathbb{R}^{n \times d}$ and $b \in \mathbb{R}^{n}$. Least square linear regression asks for $x \in \mathbb{R}^d$ so that
$$ \|A x - b\|_2$$
is minimized.
\end{definition}

\subsection{Exact solution}
Assume $b = Ax' + b'$ where $b'$ is orthogonal to column space of $A$ and $Ax'$ is projection of $b$ onto column space of $A$. Then (by Pythagoraean theorem)
$$\|Ax-b\|_2^2 = \|A(x-x') - b'\|_2^2 = \|A(x-x')\|_2^2 + \|b'\|_2^2$$ 
which is minimized when $x=x'$. The condition of $Ax'$ being projection is equivalent to
$$A^T(Ax'-b) = A^T b' = 0$$
so equivalently we have a following condition
\begin{equation}
\label{cond1}
A^T A x' = A^T b.
\end{equation}

If $(A^T A)$ is invertible (its rank is $d$), we can simply compute  $$x' = (A^T A)^{-1} A^T b.$$

\begin{definition}
Let $A = U \Sigma V^T$ be SVD of $A$. Let $\Sigma^\dagger$ be defined as diagonal matrix where $\Sigma^\dagger_{i,i} = \frac{1}{\Sigma_{i,i}}$ if $\Sigma_{i,i} \not=0$ and $0$ otherwise. We then call $A^{\dagger} = V \Sigma^{\dagger} U^T$ a \emph{pseudoinverse} of $A$.
\end{definition}

\begin{theorem}
$x' = A^\dagger b$ satisfies condition~\eqref{cond1} and has minimal $L_2$ norm among all the solutions.
\end{theorem}
\begin{proof}
First part:
$$A^TA x' = A^TA A^{\dagger} b = (V 
\Sigma^T U^T  ) (U \Sigma V^T) (V \Sigma^{\dagger} U^T) b = V \Sigma^T \Sigma \Sigma^\dagger U^T b = V \Sigma^T U^T b = A^T b$$
(note, $\Sigma^T \Sigma \Sigma^\dagger = \Sigma^T$ even though $\Sigma \Sigma^\dagger \not= I$ generally)

Second part:
any solution is of the form
$$x'' = A^\dagger b + z$$
where $A^TAz = 0$, or equivalently $V \Sigma^T \Sigma V^T z = 0$ or $\Sigma^T \Sigma V^T z = 0$ (since $V$ is orthonormal) or $\Sigma V^T z = 0$ (since $\textrm{Ker}(\Sigma^T \Sigma) = \textrm{Ker}(\Sigma)$) or $V^Tz \in \textrm{Ker}(\Sigma)$ or $z \in V \cdot \textrm{Ker}(\Sigma)$.

We have $A\dagger b = V \Sigma^\dagger U^T b \in V \cdot \textrm{Im}(\Sigma^\dagger)$.

Both $\textrm{Ker}(\Sigma)$ and $\textrm{Im}(\Sigma^{\dagger})$ are subspaces of $\mathbb{R}^{d}$ - and by closer investigation since $\Sigma$ is diagonal, both depend on zero/non-zero elements in $\Sigma_{i,i}$. Namely, $\textrm{Ker}(\Sigma) \perp \textrm{Im}(\Sigma^\dagger)$, and since $V$ is orthonormal, $V \cdot \textrm{Ker}(\Sigma) \perp V \cdot \textrm{Im}(\Sigma^\dagger)$. So by the Pythagorean theorem,
$$\|x''\|_2^2 = \|A^{\dagger} b\|_2^2 + \|z\|_2^2 \ge  \|A^{\dagger} b\|_2^2$$
which proves optimality.
\end{proof}

Downside: time to compute SVD is $\bigo(\min(n^2d, nd^2))$ which can be prohibitive.

\subsection{Approximate solution}
Instead of solving exact regression, we pick a projection $\Pi \in \mathbb{R}^{m \times n}$ and solve a problem of smaller dimensionality ($m$ instead of $n$):

$$\textrm{minimize}\quad \|\Pi A x - \Pi b\|_2$$

It is enough to use subspace embedding $\Pi$ for space spanned on columns of $A$  + single vector $b$. Thus we can pick oblivious subspace embedding for $m = \bigo(d/\varepsilon^2)$, and have

$$\forall_{x \in \mathbb{R}^d} \|A x - b\|_2 \le \|\Pi A x - \Pi b\|_2 \le (1+\varepsilon) \| A x - b\|_2.$$

Thus minimizing projected problem provides $1+\varepsilon$ approximation to original regression problem.

Total computation time is $\bigo(mn + \min(m^2d,md^2)) = \bigo(nd/\varepsilon^2 + d^3 / \varepsilon^2)$.

\section{Low rank approximation}
Consider input matrix $A \in \mathbb{R}^{n \times d}$. The goal of the low-rank approximation is the following:
find $B$ such that  $B$ has small rank and $B \approx A$.

Denote such $B = C \times D$, where $C \in \mathbb{R}^{n \times k}$ and $D \in \mathbb{R}^{k \times d}$. Motivation (assume $k$ is small)
\begin{itemize}
\item $B$ requires much less space to store: $nk + kd$ vs $nd$.
\item matrix-vector  multiplication involving $B$ is much faster: $B \cdot v$ takes $\bigo(nk + kd)$ time vs $\bigo(nd)$ time of $A \cdot v$.
\item matrix-matrix multiplication: for $X \in \mathbb{R}^{d \times m}$, $B \cdot X$ takes $\bigo(kdm + nkm)$, vs $\bigo(ndm)$ time  of $A \cdot X$
\item $A$ might have structure + noise, $B$ is denoising of $A$
\end{itemize}
\subsection{Exact solution}
We are looking at
$$ \arg \min_{B : \textrm{rank}(B)\le k} \|A-B\|$$ 
and denote it as $A_k$, best rank-$k$ approximation of $A$.

How to find such $A_k$? Following theorem holds for both $\|\cdot\|_F$ and $\|\cdot\|_2$ norms.



\begin{theorem}
Consider SVD of $A = U \Sigma V^T$. Let $\Sigma_k$ be $\Sigma$ where only $k$ largest in absolute value singular values are preserved, and every other value is zeroed.
\begin{equation}
\label{eq:bestrankk}
A_k = U \Sigma_k V^T
\end{equation}
\end{theorem}
\begin{proof}
TODO
\end{proof}

Unfortunately the time is dominated by SVD computation $\bigo(\min(nd^2,n^2d))$.

\subsection{Approximate solution}
Obtaining good approximate solution is possible for this problem, using the same framework: we project our problem to smaller dimension and hope that solution in reduced dimension approximates good solution to original problem.

Specifically, we use projection matrix $S \in \mathbb{R}^{m \times n}$, for small $m$. So $m = \bigo(k/\varepsilon^2)$ (note, $m$ is independent of dimensions of $A$, and depends on desired rank $k$). 

We mention following theorem

\begin{theorem}[ \cite{DBLP:conf/stoc/ClarksonW13} ]
There is algorithm that outputs $A'_k$ of rank $k$ in a factorized form, satisfying $\|A'_k - A\|_F \le \|A_k - A\|_F \cdot (1+\varepsilon)$. The time to compute is $\bigo(\textrm{nnz}(A) + (n+d) \textrm{poly}(k/\varepsilon))$
\end{theorem}
We skip the proof due to lack of time/space.

\section{Sparse Fourier}
Fourier transform: signal $\to$ frequencies. 
\begin{definition}
Assume $a = (a_0,\ldots,a_{n-1})$ is a signal. Let $\omega$ be $n$-th root of unity, that is $\omega = e^{-\frac{2 \pi}{n}}$. Let $F$ be such that $F_{ij} = \frac{1}{\sqrt{n}}\omega^{ij}$. Then $\hat{a} = F a$ is a (Discrete) Fourier transform of $a$.
\end{definition}

DFT can be computed in $\bigo(n \log n)$ time. However, for some applications this time can already be prohibitive. Consider a following scenario (of signal compression):
\begin{itemize}
\item Take input signal $a$ and compute $\hat{a}$.
\item Let $\hat{a}_k$ be $\hat{a}$ with only $k$ largest magnitude elements kept (rest is zeroed).
\item Output $a_k = F^{-1} \hat{a}_k$.
\end{itemize}

If we consider complexity measure of Fourier support $fs(a) = \|\hat{a}\|_0$ that is number of non-zero Fourier coefficients, then actually there is
$$ a_k = \arg \min_{x : fs(x) \le k} \|a - x\|_2$$
(proof: exercise)

If we assume that $a$ comes from real-life scenarios (photos, audio recording), then it should have only few ``strong'' frequencies, rest is noise. Since $a_k$ has much simpler representation (namely, $\hat{a}_k$ which takes $\bigo(k \log n)$ bits), this is a lossy compression scheme.

How do we compute $\hat{a}_k$ efficiently? (Assumption is that we have random access to $a$, otherwise just \emph{reading} the input would dominate the computation time.)

Simpler question: can we recover $\hat{a}$ if we know that $fs(a) = \|\hat{a}\|_0 \le k$ (so there are only $k$ non-zero frequencies)?

\subsection{Recovering sparse signal}
Assume wlog $n$ is power of two.

Define $p_{d,\ell}(x) = \sum_{i : i \bmod 2^{\ell} = d} \hat{a}_i x^i$, and for a polynomial $p(x)$ we write $\|p\|^2 = \sum_i p_i^2$.

First trick is how to estimate $\|p_{d,\ell}\|_2^2$ with additive error. Given such blackbox, we can proceed:
\begin{enumerate}
\item Define $S_{\ell} = \{i : \|p_{i,\ell}\|^2 > 0 \}$
\item Given $S_{\ell}$, compute $S_{\ell+1}$: for each $d \in S_{\ell}$, test for $e \in \{d, d+2^{\ell}\}$ whether $\|p_{e,\ell+1}\|^2 > 0$ and if so, add $e$ to $S_{\ell+1}$.
\item $p_{i,\log n} = \hat{a}_i$
\end{enumerate}

The idea is to start at level 0 and proceed. The size of each $S_{\ell}$ is bounded by $k$, thus the total number of steps (2) done is $\bigo(k \log n)$.

\bibliographystyle{alpha}
\bibliography{bib}

\end{document}


